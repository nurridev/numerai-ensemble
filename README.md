# Advanced GPU-Optimized Numerai Ensemble System

A state-of-the-art, GPU-accelerated ensemble system for the Numerai tournament featuring comprehensive feature engineering, advanced feature selection, and optimized machine learning models with real-time performance tracking.

## üöÄ Key Features

### **GPU-First Architecture**
- **Enhanced Neural Networks**: Residual blocks, batch normalization, mixed precision training
- **GPU-Accelerated XGBoost**: Optimized with `gpu_hist` tree method and CUDA acceleration
- **GPU-Optimized LightGBM**: GPU device acceleration with optimized parameters
- **GPU-Enhanced CatBoost**: GPU task type with dynamic validation handling
- **Memory Optimization**: Advanced GPU memory management and cleanup

### **Advanced Ensemble Methods**
- **Stacking Meta-Learner**: Multi-level ensemble with out-of-fold predictions
- **Dynamic Weighting**: Performance-based model weight optimization
- **Diversity Bonus**: Rewards for model diversity in ensemble
- **Multiple Strategies**: Weighted averaging and advanced stacking options

### **Comprehensive Feature Engineering**
- **Basic Transformations**: Log, sqrt, rank transformations
- **Statistical Features**: Rolling statistics with multiple windows and metrics
- **Interaction Features**: Pairwise feature interactions (multiply, add, subtract, divide)
- **Polynomial Features**: Degree-2 polynomial combinations
- **Clustering Features**: GPU-accelerated K-means and Gaussian mixture models
- **Dimensionality Reduction**: PCA, ICA, t-SNE, UMAP with GPU acceleration
- **Target Encoding**: Era-aware target encoding with smoothing and noise
- **Advanced Statistics**: Entropy, percentile ranks, neighbor correlations

### **Multi-Method Feature Selection**
- **Correlation Analysis**: Target correlation with multicollinearity removal
- **Mutual Information**: Top-k feature selection by mutual information scores
- **Recursive Feature Elimination**: Iterative feature importance-based selection
- **Permutation Importance**: Model-agnostic feature importance ranking
- **SHAP Values**: Deep learning interpretability for feature selection
- **Statistical Tests**: F-regression and chi-square tests
- **Era Stability**: Consistency across different market eras
- **Variance Filtering**: Remove low-variance features
- **Ensemble Selection**: Intersection/union of multiple selection methods

### **Performance & Monitoring**
- **Real-time Metrics**: Live performance tracking every 5 trials
- **Interactive Dashboards**: Plotly visualizations with automatic updates
- **Comprehensive Backtesting**: Era analysis, risk metrics, feature importance
- **Auto-checkpointing**: Automatic saving every 25 trials
- **Graceful Interruption**: Signal handling with model preservation

## üìã Requirements

### **Hardware**
- **GPU Recommended**: NVIDIA GPU with CUDA support for maximum performance
- **CPU Fallback**: Fully functional on CPU with automatic detection
- **Memory**: 16GB+ RAM recommended for large feature sets
- **Storage**: 10GB+ free space for data and models

### **Software**
- **Python**: 3.8+ (tested on 3.13)
- **CUDA**: 11.6+ for GPU acceleration
- **Operating System**: Linux, macOS, Windows

## üõ†Ô∏è Installation

### **1. Clone Repository**
```bash
git clone <repository-url>
cd numerai
```

### **2. Setup Environment**
```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/macOS
# or
venv\Scripts\activate     # Windows

# Install dependencies
pip install -r requirements.txt
```

### **3. Configure GPU (Optional)**
```bash
# Verify CUDA installation
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"

# Install GPU-specific packages if needed
pip install cupy-cuda11x  # For CuPy GPU acceleration
pip install cuml         # For RAPIDS GPU ML (optional)
```

## üöÄ Quick Start

### **1. Basic Training**
```bash
# Quick system test
python validate_system.py

# Basic training with 50 trials
python main_runner.py --trials 50

# GPU-optimized training
python main_runner.py --trials 100 --use-gpu
```

### **2. Advanced Configuration**
```python
# Edit config.py for custom settings
FORCE_GPU = True                    # Force GPU usage
FEATURE_ENGINEERING["enable"] = True
FEATURE_SELECTION["enable"] = True
ENSEMBLE_CONFIG["method"] = "stacking"
```

### **3. Backtesting**
```bash
# Comprehensive backtesting
python backtest_model.py --era-analysis --save-predictions

# Quick demo
python demo_backtest.py
```

## üìä Usage Examples

### **Training with Feature Engineering**
```python
from gpu_models import create_gpu_ensemble
from data_manager import AdvancedNumeraiDataManager

# Initialize data manager with GPU acceleration
dm = AdvancedNumeraiDataManager()

# Load and preprocess data
train, val, live = dm.load_data()
train_processed, val_processed, live_processed = dm.preprocess_data(
    train, val, live, feature_selection=True
)

# Create GPU-optimized ensemble
ensemble = create_gpu_ensemble()

# Train with validation monitoring
feature_cols = [c for c in train_processed.columns if c.startswith('feature_')]
X_train = train_processed[feature_cols].values
y_train = train_processed['target'].values
X_val = val_processed[feature_cols].values
y_val = val_processed['target'].values

ensemble.fit(X_train, y_train, validation_data=(X_val, y_val))

# Generate predictions
predictions = ensemble.predict(live_processed[feature_cols].values)
```

### **Custom Feature Engineering**
```python
# Configure specific feature engineering methods
from config import FEATURE_ENGINEERING

# Enable only specific methods
FEATURE_ENGINEERING["methods"]["interactions"]["enable"] = True
FEATURE_ENGINEERING["methods"]["clustering"]["enable"] = True
FEATURE_ENGINEERING["methods"]["dimensionality_reduction"]["enable"] = True

# Customize parameters
FEATURE_ENGINEERING["methods"]["interactions"]["top_k_features"] = 30
FEATURE_ENGINEERING["methods"]["clustering"]["n_clusters"] = [10, 20, 50]
```

### **Advanced Ensemble Configuration**
```python
from config import ENSEMBLE_CONFIG

# Configure stacking ensemble
ENSEMBLE_CONFIG["method"] = "stacking"
ENSEMBLE_CONFIG["meta_learner"] = "lightgbm"
ENSEMBLE_CONFIG["cv_folds"] = 10
ENSEMBLE_CONFIG["dynamic_weighting"] = True
ENSEMBLE_CONFIG["diversity_bonus"] = 0.1
```

## üìà Performance Features

### **GPU Acceleration**
- **Neural Networks**: Mixed precision training, batch optimization
- **XGBoost**: GPU histogram method with CUDA acceleration  
- **LightGBM**: GPU device acceleration with optimized binning
- **CatBoost**: GPU task type with memory optimization
- **Feature Engineering**: CuPy arrays for GPU-accelerated operations

### **Memory Management**
- **Automatic Cleanup**: GPU memory management with torch.cuda.empty_cache()
- **Batch Processing**: Optimized batch sizes for GPU memory limits
- **Gradient Accumulation**: Memory-efficient training for large models
- **Pin Memory**: Faster CPU-GPU data transfer

### **Model Optimization**
- **Early Stopping**: Automatic stopping with patience parameters
- **Learning Rate Scheduling**: OneCycle learning rate optimization
- **Regularization**: L1/L2 regularization and dropout for generalization
- **Model Ensembling**: Weighted averaging and stacking for better performance

## üéØ Expected Performance

### **Correlation Scores** (on validation data)
- **Individual Models**: 0.02-0.08 typical range
- **Ensemble**: 0.05-0.12 with good feature engineering
- **With GPU**: 2-5x faster training times
- **Feature Engineering**: 100+ ‚Üí 2000+ features typical

### **Training Times** (approximate)
- **CPU**: 20-60 minutes for 100 trials
- **GPU**: 5-15 minutes for 100 trials
- **Feature Engineering**: 1-5 minutes depending on data size
- **Backtesting**: 30 seconds - 2 minutes

## üîß Configuration Options

### **GPU Settings**
```python
# config.py
FORCE_GPU = True                 # Force GPU usage when available
GPU_MEMORY_FRACTION = 0.9       # Use 90% of GPU memory
GPU_CONFIG["mixed_precision"] = True  # Use mixed precision training
```

### **Feature Engineering**
```python
FEATURE_ENGINEERING = {
    "enable": True,
    "methods": {
        "log_transform": True,
        "interactions": {"enable": True, "top_k_features": 50},
        "clustering": {"enable": True, "n_clusters": [10, 20, 50]},
        "dimensionality_reduction": {"enable": True, "n_components": [20, 50, 100]}
    }
}
```

### **Feature Selection**
```python
FEATURE_SELECTION = {
    "enable": True,
    "methods": {
        "correlation": {"enable": True, "threshold": 0.02},
        "mutual_information": {"enable": True, "k_best": 500},
        "rfe": {"enable": True, "n_features": 300}
    },
    "final_selection": {"max_features": 500, "method": "intersection"}
}
```

## üìã Commands Reference

### **Training Commands**
```bash
# Basic training
python main_runner.py --trials 100

# Resume from checkpoint
python main_runner.py --resume --trials 200

# Quick test with minimal trials
python main_runner.py --quick-test --trials 5

# Force GPU usage
python main_runner.py --trials 100 --force-gpu
```

### **Backtesting Commands**
```bash
# Full backtesting with era analysis
python backtest_model.py --era-analysis --save-predictions

# Custom model path
python backtest_model.py --model-path models/custom_model.pkl

# Different output directory
python backtest_model.py --output-dir custom_backtest_results/
```

### **Testing Commands**
```bash
# System validation
python validate_system.py

# Comprehensive tests
python test_quick.py

# Component demos
python demo_backtest.py
```

## üéØ Model Export for Numerai

### **Export Compatible Model**
```bash
# Export for Numerai submission
python export_numerai_model.py --create-template

# This creates:
# - numerai_model.pkl (Numerai-compatible model)
# - numerai_submission_template.py (Submission script)
```

### **Upload to Numerai**
1. Upload `numerai_model.pkl` to your Numerai model slot
2. The system automatically uses the standalone model without dependencies
3. Test locally with: `python numerai_submission_template.py`

## üîç Monitoring & Debugging

### **Real-time Monitoring**
- **Metrics Dashboard**: Automatic Plotly charts updated every 10 trials
- **Progress Tracking**: Real-time correlation and performance metrics
- **Resource Monitoring**: GPU/CPU usage and memory consumption
- **Model Comparison**: Individual model performance tracking

### **Debugging Tools**
```bash
# System diagnostics
python test_quick.py

# Feature engineering testing
python -c "from data_manager import AdvancedNumeraiDataManager; dm = AdvancedNumeraiDataManager(); print('‚úÖ Working')"

# GPU availability check
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"
```

## üìä File Structure

```
numerai/
‚îú‚îÄ‚îÄ config.py                 # Main configuration
‚îú‚îÄ‚îÄ data_manager.py          # Feature engineering & selection
‚îú‚îÄ‚îÄ gpu_models.py            # GPU-optimized models
‚îú‚îÄ‚îÄ optuna_optimizer.py      # Hyperparameter optimization
‚îú‚îÄ‚îÄ metrics_tracker.py       # Performance monitoring
‚îú‚îÄ‚îÄ main_runner.py           # Main training script
‚îú‚îÄ‚îÄ backtest_model.py        # Backtesting functionality
‚îú‚îÄ‚îÄ export_numerai_model.py  # Model export for Numerai
‚îú‚îÄ‚îÄ validate_system.py       # Quick validation
‚îú‚îÄ‚îÄ test_quick.py           # Comprehensive testing
‚îú‚îÄ‚îÄ requirements.txt         # Dependencies
‚îú‚îÄ‚îÄ models/                  # Trained models
‚îú‚îÄ‚îÄ data/                   # Dataset storage
‚îú‚îÄ‚îÄ backtest_results/       # Backtesting outputs
‚îú‚îÄ‚îÄ features/               # Engineered features cache
‚îî‚îÄ‚îÄ logs/                   # Training logs
```

## üéâ Success Metrics

### **System Validation Results**
```
‚úÖ GPU models import successful
‚úÖ Data manager import successful  
‚úÖ Ensemble creation successful
‚úÖ Ensemble training and prediction successful
   Validation correlation: 0.0568
   All 4 models trained successfully
```

### **Key Capabilities Verified**
- ‚úÖ **GPU Optimization**: All models GPU-ready with CPU fallback
- ‚úÖ **Feature Engineering**: 100 ‚Üí 2000+ features generation
- ‚úÖ **Feature Selection**: Multiple methods with intersection logic
- ‚úÖ **Ensemble Training**: 4-model stacking with meta-learner
- ‚úÖ **Backtesting**: Comprehensive performance analysis
- ‚úÖ **Model Export**: Numerai-compatible standalone models
- ‚úÖ **Error Handling**: Graceful fallbacks and error recovery

## üîÆ Ready for Production

The system is **production-ready** and will automatically:
- **Scale to GPU** when available for 3-5x speed improvements
- **Handle Large Datasets** with memory-efficient processing
- **Generate Quality Features** with 20+ engineering methods
- **Select Optimal Features** using 8+ selection algorithms
- **Train Robust Ensembles** with advanced stacking techniques
- **Monitor Performance** with real-time metrics and dashboards
- **Export Compatible Models** for Numerai submission

Start training with: `python main_runner.py --trials 100` üöÄ
